{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load gmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## INPUT ##\n",
    "\n",
    "gmf_file = './GMF_complete_IM0.1+MaxDist_10km_60arcsec.csv'\n",
    "gmf_file_gmpe_rate = './GMF_complete_IM0.1+MaxDist_10km_60arcsec_gmpe_rate.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_gmf = pd.read_csv(gmf_file, header=0)\n",
    "df_gmf_gmpe_rate = pd.read_csv(gmf_file_gmpe_rate, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gmfs_median = []\n",
    "\n",
    "for i in range(len(df_gmf_gmpe_rate)):\n",
    "    gmf_median = {}\n",
    "    gmf_median['rate'] = df_gmf_gmpe_rate['rate'][i]\n",
    "    gmv = df_gmf[df_gmf.keys()[1]].values\n",
    "    gmf_median[df_gmf_gmpe_rate['gmpe'][i]] = [df_gmf[df_gmf.event_id == i][['gmv_PGA']].values,df_gmf[df_gmf.event_id == i][['gmv_SA(0.3)']].values]\n",
    "    gmfs_median.append(gmf_median)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Total Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openquake.hazardlib.imt import PGA, SA\n",
    "from openquake.hazardlib.gsim.base import RuptureContext, SitesContext, DistancesContext\n",
    "import numpy as np\n",
    "from openquake.commonlib.readinput import get_oqparam, get_site_collection, get_gsim_lt, get_exposure, get_sitecol_assetcol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## INPUT ##\n",
    "\n",
    "imts = [PGA(), SA(0.3)]\n",
    "#inter and intra values correspond to intra/inter ratio of 1.75\n",
    "#These values are used only if the GMPE is defined for TOTAL st dev\n",
    "inter = 0.496\n",
    "intra = 0.868\n",
    "vs30 = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ZhaoEtAl2006Asc()', 'ZhaoEtAl2006SInter()', 'ZhaoEtAl2006SSlab()']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oq_param = get_oqparam(\"./job_eb_cr_60_SJ2.ini\")\n",
    "\n",
    "exposure = get_exposure(oq_param)\n",
    "#Added this line\n",
    "haz_sitecol = get_site_collection(oq_param)\n",
    "sites, assets_by_site = get_sitecol_assetcol(oq_param, haz_sitecol)\n",
    "\n",
    "gsimlt = get_gsim_lt(oq_param)\n",
    "gsim_list = [br.uncertainty for br in gsimlt.branches]\n",
    "gsim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openquake.hazardlib.gsim.zhao_2006 import ZhaoEtAl2006Asc\n",
    "from openquake.hazardlib.gsim.zhao_2006 import ZhaoEtAl2006SInter\n",
    "from openquake.hazardlib.gsim.zhao_2006 import ZhaoEtAl2006SSlab\n",
    "from openquake.hazardlib import const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/openquake/GEM/oq-engine/openquake/hazardlib/gsim/zhao_2006.py:376: RuntimeWarning: divide by zero encountered in log\n",
      "  slab_term = C['SSL'] * np.log(rrup)\n"
     ]
    }
   ],
   "source": [
    "#stddevs = [const.StdDev.TOTAL]\n",
    "std_total = {}\n",
    "std_inter = {}\n",
    "std_intra = {}\n",
    "for gsim in gsim_list:\n",
    "    rctx = RuptureContext()\n",
    "    #The calculator needs these inputs but they are not used in the std calculation\n",
    "    rctx.mag = 5\n",
    "    rctx.rake = 0\n",
    "    rctx.hypo_depth = 0\n",
    "    \n",
    "    dctx = DistancesContext()\n",
    "    dctx.rjb = np.copy(np.array([0]))\n",
    "    dctx.rrup = np.copy(np.array([0]))\n",
    "    #dctx.rhypo = np.copy(np.array([0]))\n",
    "    \n",
    "    sctx = SitesContext()\n",
    "    sctx.vs30 = vs30 * np.ones_like(np.array([0]))\n",
    "    for imt in imts:\n",
    "        if gsim.DEFINED_FOR_STANDARD_DEVIATION_TYPES == \\\n",
    "            set([const.StdDev.TOTAL]):\n",
    "            gm_table, gm_stddevs = gsim.get_mean_and_stddevs(sctx, rctx, dctx, imt, stddevs)\n",
    "            std_total[gsim,imt] = gm_stddevs[0][0]\n",
    "            std_inter[gsim,imt] = gm_stddevs[0][0]*inter\n",
    "            std_intra[gsim,imt] = gm_stddevs[0][0]*intra\n",
    "        else:\n",
    "            gm_table, [gm_stddev_inter, gm_stddev_intra] = gsim.get_mean_and_stddevs(sctx, rctx, dctx, imt, [const.StdDev.INTER_EVENT, const.StdDev.INTRA_EVENT])\n",
    "            std_total[gsim,imt] = np.sqrt(gm_stddev_inter[0]**2+gm_stddev_intra[0]**2)\n",
    "            std_inter[gsim,imt] = gm_stddev_inter[0]\n",
    "            std_intra[gsim,imt] = gm_stddev_intra[0]\n",
    "            \n",
    "#print std_total\n",
    "#print std_inter\n",
    "#print std_intra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inter-event residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## INPUT ##\n",
    "realizations_inter = 5\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043270316352209529"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importance Sampling\n",
    "mean_shift = 0.75\n",
    "\n",
    "rates_inter = np.array([1./realizations_inter]*realizations_inter)\n",
    "cumulative_rates = np.cumsum(rates_inter)-rates_inter/2\n",
    "distr_values = scipy.stats.norm.ppf(cumulative_rates,loc=mean_shift,scale=1)\n",
    "p_distr_values = scipy.stats.norm.pdf(distr_values,loc=0,scale=1)\n",
    "q_distr_values = scipy.stats.norm.pdf(distr_values,loc=mean_shift,scale=1)\n",
    "weights = (p_distr_values/q_distr_values)\n",
    "rates_inter = weights/sum(weights)\n",
    "#print rates_inter\n",
    "#print distr_values\n",
    "\n",
    "#Calculate distribution mean - needs to be approximately zero\n",
    "np.mean(distr_values*rates_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get std_inter values from gmpe\n",
    "gmpe_imt = list(std_inter.keys())\n",
    "inter_residual = {}\n",
    "\n",
    "#calculate inter_residual values\n",
    "for i in range(len(gmpe_imt)):\n",
    "    stddev_inter = [std_inter[gmpe_imt[i]]]\n",
    "    inter_residual[str(gmpe_imt[i][0])+', '+str(gmpe_imt[i][1])] = stddev_inter * distr_values\n",
    "\n",
    "inter_residual['rates_inter'] = rates_inter\n",
    "\n",
    "#print inter_residual\n",
    "#print rates_inter\n",
    "#print distr_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intra-event residuals: upload values from csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## INPUT ##\n",
    "\n",
    "realizations_intra = 5\n",
    "#If No Correlation:\n",
    "mu = 0.0\n",
    "sigma = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-event residuals: No Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_coords = pd.DataFrame({'lons': sites.lons, 'lats': sites.lats})\n",
    "#intra_residual_no_coords = {}\n",
    "#intra_residual = {}\n",
    "#intra_residual['rates_intra'] = rates_intra\n",
    "\n",
    "#for x in range(len(gmpe_imt)):\n",
    "#    df_part = np.random.normal(mu, sigma, len(sites)*realizations_intra).reshape((len(sites),realizations_intra))\n",
    "#    \n",
    "#    stddev_intra = [std_intra[gmpe_imt[x]]]\n",
    "\n",
    "#    intra_residual_no_coords[str(gmpe_imt[x][0])+', '+str(gmpe_imt[x][1])] = stddev_intra * df_part\n",
    "#    intra_residual[str(gmpe_imt[x][0])+', '+str(gmpe_imt[x][1])] = np.concatenate([df_coords.values,intra_residual_no_coords[str(gmpe_imt[x][0])+', '+str(gmpe_imt[x][1])]], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intra-event residuals: Spatial and Cross Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## INPUT ##\n",
    "intra_matrices_file = './Matrix_Intra_Res_SJ_60arcsec_10k_seed1_trunc3_withfilter_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "df_0 = pd.read_csv(intra_matrices_file+str(imts[0])+'.csv', nrows = 2, header=None)\n",
    "number_cols = len(df_0.columns)\n",
    "\n",
    "#Find indeces of rows to extract from Matrices\n",
    "df = pd.read_csv(intra_matrices_file+str(imts[0])+'.csv', usecols=[0,1], header=None)\n",
    "\n",
    "coords_matrix = np.array(df)\n",
    "exposure_coords = np.array(list(zip(*[sites.lons,sites.lats])))\n",
    "rows_to_extract = np.argmin(cdist(coords_matrix,exposure_coords,'sqeuclidean'),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Multiply entire table (1000matrices) by \n",
    "#Only works when rates are equal (Not applicable for IS)!!!\n",
    "\n",
    "intra_residual = {}\n",
    "#If rates are all equal\n",
    "rates_intra = [1./realizations_intra]*realizations_intra\n",
    "intra_residual['rates_intra'] = rates_intra\n",
    "\n",
    "#np.random.seed(99)\n",
    "#cols = np.random.choice(range(2,number_cols), realizations_intra, replace=False)\n",
    "cols = np.array(range(2,number_cols))\n",
    "#intra_residual['rates_intra']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "intra_residual_no_coords = {}\n",
    "\n",
    "df_coords = pd.DataFrame({'lons': sites.lons, 'lats': sites.lats})\n",
    "\n",
    "for x in range(len(gmpe_imt)):\n",
    "    IMT = str(gmpe_imt[x][1])\n",
    "\n",
    "    file_name = intra_matrices_file+str(IMT)+'.csv'\n",
    "    df = pd.read_csv(file_name, usecols=cols, header = None)\n",
    "    df_part = df.loc[rows_to_extract].reset_index(drop=True)\n",
    "    \n",
    "    #get std_intra values from gmpe\n",
    "    stddev_intra = [std_intra[gmpe_imt[x]]]\n",
    "    intra_residual_no_coords[str(gmpe_imt[x][0])+', '+str(gmpe_imt[x][1])] = stddev_intra * df_part.values\n",
    "    \n",
    "    intra_residual[str(gmpe_imt[x][0])+', '+str(gmpe_imt[x][1])] = np.concatenate([df_coords.values,intra_residual_no_coords[str(gmpe_imt[x][0])+', '+str(gmpe_imt[x][1])]], axis=1)\n",
    "\n",
    "#intra_residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sum median with residuals and save .csv file - For ruptures after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## INPUT ##\n",
    "\n",
    "csv_gmf_file = './GMF_results_seed1.csv'\n",
    "csv_rate_gmf_file = './GMF_results_seed1_rate.csv'\n",
    "seed=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "U16 = np.uint16\n",
    "U32 = np.uint32\n",
    "U64 = np.uint64\n",
    "F32 = np.float32\n",
    "\n",
    "num_coords = len(sites)\n",
    "#num_gmfs = len(gmfs_median)*len(inter_residual['rates_inter'])*len(intra_residual['rates_intra'])\n",
    "num_gmfs = 1*len(inter_residual['rates_inter'])*len(intra_residual['rates_intra'])\n",
    "\n",
    "lst_=[]\n",
    "for sid in range(num_coords):\n",
    "    list_a = []\n",
    "    for eid in range(int(num_gmfs)):\n",
    "        list_a.append((sid+eid*num_coords, sid+eid*num_coords + 1))\n",
    "    a = np.array(list_a, np.dtype([('start', U32), ('stop', U32)]))\n",
    "    lst_.append(a)\n",
    "lst = np.array(lst_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/openquake/oqdata'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openquake\n",
    "ds = openquake.baselib.datastore.get_datadir()\n",
    "last_id = openquake.baselib.datastore.get_last_calc_id()\n",
    "ds\n",
    "#last_id\n",
    "\n",
    "from openquake.commonlib import logs\n",
    "\n",
    "logs.dbcmd('import_job', last_id, 'event_based',\n",
    "               'eb_test_hdf5', 'ccosta', 'complete', \n",
    "               None, '/home/openquake/oqdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Unable to create link (Name already exists)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4684816db765>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gmf_data/indices'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GEM/.env/py35/lib/python3.5/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36mcreate_dataset\u001b[0;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mdset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GEM/oq-engine/openquake/baselib/hdf5.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, path, obj)\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__setitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpyclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# make sure it is fully saved\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/tmp/pip-huypgcah-build/h5py/_objects.c:2840)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/tmp/pip-huypgcah-build/h5py/_objects.c:2798)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/GEM/.env/py35/lib/python3.5/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHLObject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlcpl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftLink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/tmp/pip-huypgcah-build/h5py/_objects.c:2840)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper (/tmp/pip-huypgcah-build/h5py/_objects.c:2798)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5o.pyx\u001b[0m in \u001b[0;36mh5py.h5o.link (/tmp/pip-huypgcah-build/h5py/h5o.c:3895)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unable to create link (Name already exists)"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from openquake.baselib import hdf5\n",
    "import numpy as np\n",
    "from openquake.commonlib import source\n",
    "from openquake.commonlib.readinput import get_oqparam, get_gsim_lt, get_exposure, get_sitecol_assetcol\n",
    "\n",
    "\n",
    "f = hdf5.File('./mytestfile5.hdf5', 'a')\n",
    "\n",
    "data = lst\n",
    "dt = data[0].dtype\n",
    "dtype = h5py.special_dtype(vlen=dt)\n",
    "dset = f.create_dataset('gmf_data/indices', (len(sites),), dtype, compression=None)\n",
    "for i, val in enumerate(lst):\n",
    "    dset[i] = val\n",
    "    \n",
    "gmdata_dt = np.dtype([('PGA', F32), ('SA(0.3)', F32), ('events', U32), ('nbytes', U32)])\n",
    "dset1 = f.create_dataset('gmdata', (1,), dtype=gmdata_dt)\n",
    "dset1['events'] = int(num_gmfs)\n",
    "dset1['PGA'] = 0.03\n",
    "dset1['SA(0.3)'] = 0.031\n",
    "\n",
    "cinfo = source.CompositionInfo.fake(gsimlt)\n",
    "\n",
    "events_dt = np.dtype([('eid', U64), ('rup_id', U32), ('grp_id', U16), ('year', U32), ('ses', U32), ('sample', U32)])\n",
    "dset2 = f.create_dataset('events', (num_gmfs,), dtype=events_dt)\n",
    "dset2['eid'] = np.arange(0,num_gmfs)\n",
    "dset2['ses'] = np.ones(num_gmfs)\n",
    "dset2['rup_id'] = np.ones(num_gmfs)\n",
    "dset2['year'] = np.ones(num_gmfs)\n",
    "\n",
    "f['csm_info'] = cinfo\n",
    "f['sitecol'] = sites\n",
    "f['oqparam'] = oq_param\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "#row1 = 'rlzi,sid,eid,gmv_'+str(imts[0])+',gmv_'+str(imts[1])+ '\\n'\n",
    "#with open(csv_gmf_file, 'w') as text_fi:\n",
    "#    text_fi.write(row1)\n",
    "    \n",
    "#row1_rate = 'event_id,rate' + '\\n'\n",
    "#with open(csv_rate_gmf_file, 'w') as text_fi_2:\n",
    "#    text_fi_2.write(row1_rate)\n",
    "\n",
    "num_coords = len(sites)\n",
    "eid = -1\n",
    "num_intra_matrices = len(df_0.columns)-2\n",
    "\n",
    "zip_intra = {}\n",
    "for al in range(len(gsim_list)):\n",
    "    for c in range(len(imts)):\n",
    "        zip_intra[str(gsim_list[al])+', '+str(imts[c])] = list(zip(*intra_residual[str(gsim_list[al])+', '+str(imts[c])]))\n",
    "\n",
    "first_row = 0\n",
    "I = len(imts)\n",
    "shape_val = num_gmfs*num_coords\n",
    "gmv_data_dt = np.dtype([('rlzi', U16), ('sid', U32), ('eid', U64), ('gmv', (F32, (I,)))])\n",
    "with hdf5.File('./mytestfile5.hdf5', 'a') as f:\n",
    "    dset3 = f.create_dataset('gmf_data/data', (shape_val,), dtype=gmv_data_dt, chunks=True)\n",
    "\n",
    "\n",
    "#with open(csv_gmf_file, 'a') as text_fi:\n",
    "#    aa = csv.writer(text_fi, delimiter=',')\n",
    "    with open(csv_rate_gmf_file, 'a') as text_fi_2:\n",
    "        ab = csv.writer(text_fi_2, delimiter=',')\n",
    "\n",
    "        #for a in range(len(gmfs_median)):\n",
    "        for a in range(0,1):\n",
    "            index_gmf = a\n",
    "            keys_gmfs = gmfs_median[index_gmf].keys()\n",
    "            gmf_gmpe = [i for i in keys_gmfs if i in gsim_list][0]\n",
    "            \n",
    "            for d in range(len(inter_residual['rates_inter'])):\n",
    "                for e in range(len(intra_residual['rates_intra'])):\n",
    "                    np.random.seed(seed+a+d*1000+e*10000)\n",
    "                    aleatoryIntraMatrices = np.random.choice(range(num_intra_matrices), realizations_intra, replace=False)\n",
    "                    eid += 1\n",
    "                    col1_txt = np.zeros((num_coords,1)).flatten()\n",
    "                    col2_txt = np.arange(num_coords)\n",
    "                    col3_txt = np.full((1,num_coords), eid, dtype=int)[0]\n",
    "                    gmf_to_txt = np.transpose(np.array([col1_txt,col2_txt,col3_txt]))\n",
    "                    rate =  gmfs_median[index_gmf]['rate'] * inter_residual['rates_inter'][d] * intra_residual['rates_intra'][e]\n",
    "                    for c in range(len(imts)):\n",
    "                        gmf_total_part = {}                \n",
    "                        gmv = gmfs_median[index_gmf][gmf_gmpe][c]\n",
    "                        gmf_total_part[gmf_gmpe,imts[c],d,e] = np.exp(np.log(gmv)\n",
    "                                                        + inter_residual[str(gmf_gmpe)+', '+str(imts[c])][d]\n",
    "                                                        + np.array(zip_intra[str(gsim_list[al])+', '+str(imts[c])][2+aleatoryIntraMatrices[e]]).reshape((-1,1)))\n",
    "\n",
    "                        gmf_to_txt = np.c_[gmf_to_txt,gmf_total_part[gmf_gmpe,imts[c],d,e].flatten()]\n",
    "                       \n",
    "                    #aa.writerows(map(lambda t: (\"%i\" % t[0], \"%i\" % t[1], \"%i\" % t[2], \"%.3f\" % t[3], \"%.3f\" % t[4]), gmf_to_txt))\n",
    "                    ab.writerow([eid,rate])\n",
    "                    \n",
    "                    #Save in hdf5\n",
    "                    dset3['rlzi',first_row:first_row+num_coords] = gmf_to_txt[:,0]\n",
    "                    dset3['sid',first_row:first_row+num_coords] = gmf_to_txt[:,1]\n",
    "                    dset3['eid',first_row:first_row+num_coords] = gmf_to_txt[:,2]\n",
    "                    dset3['gmv',first_row:first_row+num_coords] = gmf_to_txt[:,3:5]\n",
    "                    first_row = first_row+num_coords\n",
    "    f.close()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
